###  Constraint Based Logic Programming

It's not always apparent when constraints can be used efectively to solve a problem. This section uses a number of examples which can be used as patterns to evaluate the use of constraints as an effective solution. The techniques decscribed in the section should be applicable to any CLP implementation depending on the domains supported.

#### Transitive Closure

"The [transitive closure] of a binary relation *R* on a set *X* is the smallest relation on *X* that contains *R* and is transitive." For example if *X* is a set of cities and the relation *R* between two cities means there is a direct flight between them, the transitive closure is the relation defining two cities which are connected by one or more flights.

Among other things, the general notion of tranistive closure can be used to determine reachability in a graph (*R* defines nearest neighbour) or the functional dependence between attributes in a database (*R* specifies which attributes are a direct consequence of other attributes).

An implementation of transitive closure using Prolog lists is fairly straight forward:
.pl
	closure(In, RList, Out) :-
		clos(RList, In, UOut),  % generate and test
		sort(UOut,Out).         % sort for presentation purposes

	clos(Rs, In, Out) :-
		member((I -> O), Rs),   % select R: I -> O
		subset(I, In),          % if I is a subset of In and
		not(subset(O, In)),     % O is not already a subset of In, 
		union(O, In, In1), !,   % add O to In
		clos(Rs, In1, Out).     % and repeat
	clos(Rs, Out, Out).         % quit

Here the binary relation is assumed to be of the form `I -> O`. As an example, here's a list of functional dependancies of a hypothetical database whose attributes are labelled `a` to `k`.
.pl
	fdList([
		[a] -> [b, e, f, g],
		[a, c, d, i] -> [h],
		[c, d] -> [j],
		[c, d, f] -> [k],
		[b] -> [g],
		[c, f] -> [a, b, e],
		[a, c] -> [d],
		[a, d] -> [c],
		[e, g] -> [b]
	]).
This says the value of attribute `a` directly determines the values of attributes `b`, `e`, `f`, and `g`. The collective values of attributes `a`, `c`, `d`, and `i` determine the value of attribute `h`, etc.

With this defintion of *R*, `closure/3` can be used to ask "What is the set of attrributes that can uniquely (directly or indirectly) be determind from the value of `a`?". (the binding of `FDs` has been omitted for clarity.)
eg
	?- fdList(FDs), closure([a], FDs, O).
	O = [a,b,e,f,g].
Other examples:
eg
	?- fdList(FDs), closure([c,d,f], FDs, O).
	O = [a,b,c,d,e,f,g,j,k].

	?- fdList(FDs), closure([c,d], FDs, O).
	O = [c,d,j].

	?- fdList(FDs), closure([b], FDs, O).
	O = [b,g].

	?- fdList(FDs), closure([a,c,d,i], FDs, O).
	O = [a,b,c,d,e,f,g,h,i,j,k].

Implementing transitive closure using constraints requires a different pespective. The general pattern for dealing with this class of combinatoric problems is:
1..
	* Set up the data structures and the declarations for the principle variables. 
	* Set up all the constraints. This part should be strictly deterministic; any non-determinism should be postponed until step 3.
	* Proceed to the enumeration of constrained variables, or other non-deterministic bits. The bulk of the execution time will normally be spent in this section on difficult combinatorial problems, so it should generally not be doing anything other than enumeration. Step 1 should have built data structures so that it is easy to extract the enumeration variables.
	* Once the solution is obtained, there is usually some code required to capture the answer in a suitable form, e.g., for presentation on the terminal or writing to a file.

We'll assume for now that the same `fdList` fact will be used for defining the relation, and the input and output will be the same as the initial Prolog program. However, in step 2 we need to build an intermediate data structure containing constrained variables that can be "enumerated" in step 3. For this purpose, a list of `label=Boolean` pairs will be used. `label` is the attribute name, e.g., `a`,`b`,`c`, etc. The meaning of the associated `Boolean` is as follows: if the variable is `1`, `label` is "reachable", otherwise it isn't. Constraints will be generated from the list of `I->O`'s. For example, `[c, f] -> [a, b, e]` will generate the equivalent of the following constraints:
eg
	{
		(C and F -> A),
		(C and F -> B),
		(C and F -> E)
	}
where `A` is the boolean variable associated with `a`, `B` with `b`, and so on. "`->`" is reused as the constraint operator for boolean implication (`=<` could also be used).

aside> Aside: The somewhat unnatural use of parentheses in the `{}` constraint expression is necessitated by the ruse of the '`->`' operator which has lower binding precedence than '`,`' in most standard Prologs.

The enumeration step (3) just requires that the boolean variable associated with each input `label` is set to `1`; the constraint network will do the rest by propagating 1's to each of the "reachable" labels. Then the output list is just the list of `label`'s whose `Boolean`'s are 1. This constraint based solution looks like:

.pl
	closureCI(In, Rs, Out) :-
		extract_labels(Rs, [], Labels),  % extract labels from Rs and associate booleans
		clos_constrain(Rs,Cs),           % define constraints
		set_bools_(In,Cs),               % initial set
		get_names_(Cs,UOut),             % retrieve all "reachable"
		sort(UOut,Out).                  % sort for presentation purposes

	extract_labels([], Labels, Labels).
	extract_labels([I->O|FDList], In, Labels) :-
		extract_labels(I,In,In1),
		extract_labels(O,In1,In2), !,
		extract_labels(FDList, In2, Labels).
	extract_labels([L|List], In, Labels) :-
		memberchk(L=_,In), !,  % already defined so skip
		extract_labels(List, In, Labels).
	extract_labels([L|List], In, Labels) :-
		extract_labels(List, [L=B|In], Labels).  % add label definition

	clos_constrain([],_).
	clos_constrain([I->O|Rs],Cs) :-
		clos_ins_(I,IExp,Cs),
		clos_outs(O,IExp,Cs),
		clos_constrain(Rs,Cs).

	clos_ins_([A],AV,Cs) :- memberchk(A=AV,Cs),!.
	clos_ins_([A|As],AV and B,Cs) :- memberchk(A=AV,Cs), !, clos_ins_(As,B,Cs).

	clos_outs([],IExp,Cs).
	clos_outs([O|Os],IExp,Cs) :-
		memberchk(O=OV,Cs), !,
		{IExp -> OV},
		clos_outs(Os,IExp,Cs).

	set_bools_([],FDs).
	set_bools_([N|Ns],FDs) :-
		memberchk(N=1,FDs), !,  % finds N=Bn and unifies Bn with 1, use deterministic version
		set_bools_(Ns,FDs).

	get_names_([],[]) :- !.
	get_names_([N=0|FDs],Out) :- !,       % false (not set)
		get_names_(FDs,Out).
	get_names_([N=1|FDs],[N|Out]) :-      % true
		get_names_(FDs,Out).

Using the same `fdList` data (top-level outut for `FDs` omitted for clarity):
eg
	?- fdList(FDs), closureCI([a], FDs, O).
	FDs = ....
	O = [a, b, e, f, g].

	?- fdList(FDs), closureCI([c,d,f], FDs, O).
	FDs = ....
	O = [a, b, c, d, e, f, g, j, k].

	?- fdList(FDs), closureCI([c,d], FDs, O).
	FDs = ....
	O = [c, d, j].

	?- fdList(FDs), closureCI([b], FDs, O).
	FDs = ....
	O = [b, g].

	?- fdList(FDs), closureCI([a,c,d,i], FDs, O).
	FDs = ....
	﻿O = [a, b, c, d, e, f, g, h, i, j, k].

But notice that the amount of code has increased, in part because the initial Prolog version was able to take advantage of the predicates `subset/2` and `union/3` in the `lists` utility package. The constraint version doesn't need these, but instead constraints have to be constructed from the data (steps 1 and 2); a step which was unnecessary in the original version. And it also needs to build a suitable output form (step 4) from the constraint data structure (list of `label=Boolean`'s).

Some of this code also depends on the format of the original data, i.e., `fdList/1`. It's worth noting that it's not much more difficult to hand build the constraints rather than use the original data (note that there is no need to declare the constrained variables as `boolean` since that's implied by the operators in the constraint expressions):
.pl
	fdConstraints([a=A,b=B,c=C,d=D,e=E,f=F,g=G,h=H,i=I,j=J,k=K]) :-
		{	(A -> B), (A -> E), (A -> F), (A -> G),
			(A and C and D and I -> H),
			(C and D -> J),
			(C and D and F -> K),
			(B -> G),
			(C and F -> A), (C and F -> B), (C and F -> E),
			(A and C -> D),
			(A and D -> C),
			(E and G -> B)
		}.
and now (this time exposing the internal constraints data structure):
eg
	?- fdConstraints(Cs), set_bools_([c,d,f],Cs), get_names_(Cs,O).
	﻿Cs = [a=1, b=1, c=1, d=1, e=1, f=1, g=1, h=0, i=0, j=1, k=1],
	O = [a, b, c, d, e, f, g, j, k].

So the bulk of the work has shifted to the construction of the constraint network. But, for the most part, this code is deterministic and relatively easy to write, test, and debug. A simple test procedure is just to specify one or more correct answers which, of course, must satisfy the constraints.

There is also a potential significant upside on performance if constraints are used. Boolean satisfiability is the paradigm NP-complete problem: in the worst case one may need to explore `2**N` branches if there are `N` boolean variables to enumerate. The effect of constraints is to reduce this to `2**M` where `M<N`, at the cost of value propagation in the constraint network. But every constraint that forces a variable (thereby avoiding one choice) halves the overall cost.

<#TableOfContents>

#### Structural Analysis of Petri nets

[Petri nets] are widely used to model discrete control systems, transaction systems, and communications protocols. A Petri net consists of a net together with a marking which represents the state of the net. The net is a bipartite graph consisting of places and transitions connected by directed arcs. Arcs connect places to transitions and transitions to places. A marking is a distribution of tokens over the places. Any transition may have input places (where the arc is directed from the place to the transition) and output places (arc from transition to place). A transition can fire if all of its input
places have at least one token; firing a transaction removes a token from each input and puts a token into each output place. In general the evolution of the state is non-deterministic as there may be many transitions that can fire for any given marking.

Structural analysis studies properties that depend only on the topology of the network independent of marking. Of particular
importance are structural properties that determine behavioural possibilities, e.g., deadlock. One such property is a *siphon* A Petri net (and hence the process it is modelling) is deadlock free if none of the siphons in the net can be emptied of tokens. Thus, a sub-problem of the structural analysis of a Petri net is to identify all the siphons.

A siphon is defined as a (non-empty) list of the places such that every transition that inputs to it also outputs to it. So for a siphon {`S`} and each transition {`t`}:
`	`{`outputs(t) nn S <> O/ -> i\nputs(t) nn S <> O/`}
or equivalently:
`	`{`i\nputs(t) nn S = O/ -> outputs(t) nn S = O/`}
For each place create a boolean variable `B` interpreted as `B=0` means that it is in the siphon S. Then for each transition `I -> 0`,  map `I` and `O` to their respective associated list of booleans `IB` and translate the above condition to `IB -> OB`. 

Further, let's constrain a siphon to be at least two places but not all the places using the [`cardinality`] predicate defined earlier. The program:
.pl
	siphon(List_of_names):-
		places(Places),
		map_table(Places, Map, Bs),
		findall(T, transition(T), Tlist),   % collect transitions in a list
		map_transitions(Tlist, Map),        % (step 2) define constraints:  definition of siphon
		length(Places,Np),                  %  and a siphon is not [] or all the places
		cardinality(Bs,2,Np-1),
		enumerate(Bs),                      % (step 3)
		select_names(Map, List_of_names).   % (step 4)

	% build 'symbol table' of Place=Boolean for mapping place names, export boolean vector
	map_table([],[],[]).
	map_table([P|Ps], [P=B|Ms], [B|Bs]):- 
		map_table(Ps,Ms,Bs).

	map_transitions([],_).
	map_transitions([I -> O|Ts], Map):-
		map_places(I,Map,IB),  % IB is symbolic conjunction of I
		map_places(O,Map,OB),  % OB is symbolic conjunction of O
		{IB -> OB},            % for any transition, no inputs in siphon implies no outputs in siphon
		map_transitions(Ts, Map).

	map_places([],_,1).  % conjunction so [] is 'true'
	map_places([P|Ps], Map, PB and B):- 
		memberchk(P=PB, Map), !,
		map_places(Ps,Map,B).

	% convert solution to list of place names
	select_names([], []).
	select_names([P=0|Ms], [P|Ps]) :- !, select_names(Ms,Ps).  % B=0 means P is in the siphon
	select_names([P=1|Ms], Ps)     :- select_names(Ms,Ps).     % P not in the siphon
An example Petri net:
.pl
	places([a,b,c,d,e,f,g,h,i,j,k]).

	transition([a,d] -> [c]).
	transition([c] -> [b,d]).
	transition([b] -> [a]).
	transition([b] -> [d,e,h]).
	transition([e] -> [f,i]).
	transition([f] -> [g]).
	transition([g] -> [e]).
	transition([i,j] -> [h,k]).
	transition([k] -> [j]).
and its siphons:
eg
	?- siphon(X),X\=[],writeln(X),fail.
	[a,b,c,d,e,f,g,h,i,k]
	[a,b,c,d,e,f,g,h,i]
	[a,b,c,d,e,f,g,h,j,k]
	[a,b,c,d,e,f,g,i,j,k]
	[a,b,c,d,e,f,g,i,k]
	[a,b,c,d,e,f,g,i]
	[a,b,c,d,e,f,g,j,k]
	[a,b,c,d,e,f,g]
	[a,b,c,d,h,j,k]
	[a,b,c,d,j,k]
	[a,b,c,d]
	[a,b,c,e,f,g,h,i,j,k]
	[a,b,c,e,f,g,h,i,k]
	[a,b,c,e,f,g,h,i]
	[a,b,c,e,f,g,h,j,k]
	[a,b,c,e,f,g,i,j,k]
	[a,b,c,e,f,g,i,k]
	[a,b,c,e,f,g,i]
	[a,b,c,e,f,g,j,k]
	[a,b,c,e,f,g]
	[a,b,c,h,j,k]
	[a,b,c,j,k]
	[a,b,c]
	[b,c,d,e,f,g,h,i,j,k]
	[b,c,d,e,f,g,h,i,k]
	[b,c,d,e,f,g,h,i]
	[b,c,d,e,f,g,h,j,k]
	[b,c,d,e,f,g,i,j,k]
	[b,c,d,e,f,g,i,k]
	[b,c,d,e,f,g,i]
	[b,c,d,e,f,g,j,k]
	[b,c,d,e,f,g]
	[b,c,d,h,j,k]
	[b,c,d,j,k]
	[b,c,d]
	[j,k]
	false.
A minimal siphon is a siphon which doesn't contain another siphon. To find the list of minimal siphons from the list of siphons:
.pl
	minimal_siphons(Siphons,Minimal) :-
		minimal_siphons_(Siphons,[],Minimal).

	minimal_siphons_([],Minimal,Minimal).
	minimal_siphons_([S|Ss],Mins,Minimal) :-
		contains_subset(Ss,S), !,              % succeeds if a member of Ss is a subset of S
		minimal_siphons_(Ss,Mins,Minimal).
	minimal_siphons_([S|Ss],Mins,Minimal) :-   % S must be a minimal siphon
		minimal_siphons_(Ss,[S|Mins],Minimal).

	contains_subset([S1|Ss],S) :-
		subset(S1,S),!.
	contains_subset([S1|Ss],S) :-
		contains_subset(Ss,S).
and use `findall/3`:
eg
	?- findall(S,siphon(S),Ss),minimal_siphons(Ss,Mins).
	﻿Ss = [[a, b, c, d, e, f, g, h, i, k], [a, b, c, d, e, f, g, h, i], [a, b, c, d, e, f, g, h, j, k], [a, b, c, d, e, f, g, i, j, k], [a, b, c, d, e, f, g, i, k], [a, b, c, d, e, f, g, i], [a, b, c, d, e, f, g, j, k], [a, b, c, d, e, f, g], [a, b, c, d, h, j, k], [a, b, c, d, j, k], [a, b, c, d], [a, b, c, e, f, g, h, i, j, k], [a, b, c, e, f, g, h, i, k], [a, b, c, e, f, g, h, i], [a, b, c, e, f, g, h, j, k], [a, b, c, e, f, g, i, j, k], [a, b, c, e, f, g, i, k], [a, b, c, e, f, g, i], [a, b, c, e, f, g, j, k], [a, b, c, e, f, g], [a, b, c, h, j, k], [a, b, c, j, k], [a, b, c], [b, c, d, e, f, g, h, i, j, k], [b, c, d, e, f, g, h, i, k], [b, c, d, e, f, g, h, i], [b, c, d, e, f, g, h, j, k], [b, c, d, e, f, g, i, j, k], [b, c, d, e, f, g, i, k], [b, c, d, e, f, g, i], [b, c, d, e, f, g, j, k], [b, c, d, e, f, g], [b, c, d, h, j, k], [b, c, d, j, k], [b, c, d], [j, k]],
	Mins = [[j, k], [b, c, d], [a, b, c]].

A complementary concept to a siphon in a Petri net is a trap: a (non-empty) list of the places such that every transition that outputs to it also inputs from it. If a trap ever contains tokens, it will always contain tokens. The program above can be trivially converted to finding traps by reversing the transition constraint, i.e., `{OB -> IB}` instead of `{IB -> OB}`.

For more on siphons and traps in Petri nets see [Siphons and Traps Structural Analysis Techniques Behaviour of a Petri Nets].

<#TableOfContents>

#### "Money" Cryptographic Puzzle

There are numerous constraint puzles in the integer domain and one of the classics is "SEND MORE MONEY" which finds values corresponding to the letters in the sum:
		 SEND
		+MORE
		-----
		MONEY
Leading 0's are not allowed so the constraints are defined by:
.pl
	sendmoremoney([S,E,N,D,M,O,R,Y]) :-
		[S,M]::integer(1,9),           % no leading 0's on any number
		[E,N,D,O,R,Y]::integer(0,9),
		distinct([S,E,N,D,M,O,R,Y]),   % all variables distinct
		{	          1000 *(S+M)+ 100 *(E+O)+ 10 *(N+R) + (D+E) ==
			10000*M + 1000 *   O + 100 *   N + 10 *   E  +  Y }.

	distinct([]) .
	distinct([X|Xs]):- distinct(Xs,X), distinct(Xs).
	
	distinct([],_).
	distinct([X|Xs],Y):- {X<>Y}, distinct(Xs,Y).

The constraints by themselves are insufficient to produce a solution:
eg
	?- sendmoremoney([S,E,N,D,M,O,R,Y]).
	﻿S = 9,
	M = 1,
	O = 0,
	E::integer(4, 7),
	N::integer(5, 8),
	D::integer(2, 8),
	R::integer(2, 8),
	Y::integer(2, 8).
But solving (searching) for `E` produces the answer:
eg
	?- sendmoremoney([S,E,N,D,M,O,R,Y]),solve(E).
	S = 9,
	E = 5,
	N = 6,
	D = 7,
	M = 1,
	O = 0,
	R = 8,
	Y = 2 ;
	false.
The result may depend on which letter is chosen, for example:
eg
	?- sendmoremoney([S,E,N,D,M,O,R,Y]),solve(R).
	﻿S = 9,
	M = 1,
	O = 0,
	R = 8,
	E::integer(5, 6),
	N::integer(6, 7),
	D::integer(2, 7),
	Y::integer(2, 7).
When in doubt, `solve/1` can be used on more than one (or all) of the variables:
eg
	?- sendmoremoney([S,E,N,D,M,O,R,Y]),solve([S,E,N,D,M,O,R,Y]).
	S = 9,
	E = 5,
	N = 6,
	D = 7,
	M = 1,
	O = 0,
	R = 8,
	Y = 2 ; 
	false.

`solve/1` is a general purpose predicate that can be used for both `real` and `integer` (including `boolean`) intervals. (If the number of intervals and their domain sizes  are small, `enumerate/1` may be slightly more efficient.) `solve` is an effective method of searching for point solutions as it avoids splitting intervals on solutions. Splitting on a solution is undesirable in that it can result in multiple answers for the same solution. (Other applications discussed below are better addressed by `splitsolve/1` which does a general bifurcating search.) 

The implmentation of `<>` in CLP(BNR) is not particularly effective since domains cannot be a set of disjoint ranges as in some other finite domain constraint systems. Problems that fit into this problem space and where efficiency is a concern, would be better solved by a different CLP package.

<#TableOfContents>

#### Magic Series

A "magic series" of order `N` can be thought of as finding a sequence `T = [M0, M1, ... Mn]` such that `T` contains `M0` occurences of `0`, `M1` occurences of `M1`, ... , and `Mn` occurences of `Mn`. The first problem is to define what `M` occurences of a member of a list means in terms of constraints. One possibilty is to use the cardinality concept to count boolean variables which indicate whether the corresponding item is an occuence or not:

.pl
	occurrences(X,List,N):- 
		N::integer(0,_),           % number of occurences is positive
		occurrenceBs_(List,X,Bs),  % create booleans which are true if corresponding list element is an occurence
		sym_sum(Bs,S), {N == S}.   % sum booleans to find number of occurences

	occurrenceBs_([], _, []).
	occurrenceBs_([X|Xs], Y, [B|Bs]) :- 
		{B == (X==Y)},             % B is true if X is an occurence of Y
		occurrenceBs_(Xs,Y,Bs).

	sym_sum([], 0).
	sym_sum([X|Xs], X+S) :- sym_sum(Xs, S).
Note the boolean constraint `{B == (X==Y)}` which means `B` is `1` (true) if `X` (a member of `List`) is numerically equal to `Y` even though `X` and `Y` are unknown. Given this defintion of `occurences/3`, it's straight forward to specify the constraints of a magic series of length `N`:
.pl
	magic_series1(N, Ks) :- 
		length(Ks,N), Ks::integer(0,N),
		magic_(Ks, 0, Ks).

	magic_([], N, _).
	magic_([K|Ks],N,KS):- 
		occurrences(N, KS, K),
		N1 is N+1,
		magic_(Ks,N1,KS).
For `N=4` and `N=10`:
eg
	?- magic_series1(4,M),enumerate(M).
	M = [1,2,1,0] ;
	M = [2,0,2,0] ;
	false.
	
	?- magic_series1(10,M),enumerate(M).
	M = [6,2,1,0,0,0,1,0,0,0] ;
	false.

	?- clpStatistics,magic_series1(10,M),findall(M,enumerate(M),Ms),clpStatistics(SS).
	﻿M = [_98516, _98522, _98528, _98534, _98540, _98546, _98552, _98558, _98564, _98570],
	Ms = [[6, 2, 1, 0, 0, 0, 1, 0, 0, 0]],
	SS = [userTime(0.3209630000000061), gcTime(0.002), globalStack(326608/1048544), trailStack(39512/264168), localStack(2432/118648), inferences(1763531), narrowingOps(37429), narrowingFails(73), node_count(300), max_iterations(566/3000)],
	_98516::integer(0, 10),
	_98570::integer(0, 10),
	_98522::integer(0, 10),
	_98564::integer(0, 10),
	_98528::integer(0, 10),
	_98558::integer(0, 10),
	_98534::integer(0, 10),
	_98552::integer(0, 10),
	_98540::integer(0, 10),
	_98546::integer(0, 10).
Looking at the statistics for `N=10`, we see that finding the single solution tooke 37429 narrowing operations with 73 failures. In thinking about the problem, you may have discovered that the sum of the the series must equal its length (the total number of elements). If this fact is used as a redundant constraint:
.pl
	magic_series2(N, Ks) :-
		length(Ks,N), Ks::integer(0,_),
		sym_sum(Ks, S), {S==N},            % <== redundant constraint
		magic_(Ks, 0, Ks).
Now:
eg
	?- clpStatistics,magic_series2(10,M),findall(M,enumerate(M),Ms),clpStatistics(SS).
	﻿M = [_86342, _86348, _86354, _86360, _86366, _86372, _86378, _86384, _86390, _86396],
	Ms = [[6, 2, 1, 0, 0, 0, 1, 0, 0, 0]],
	SS = [userTime(0.11804099999994833), gcTime(0.001), globalStack(276184/1048544), trailStack(15504/526312), localStack(2432/118648), inferences(628928), narrowingOps(11534), narrowingFails(41), node_count(310), max_iterations(560/3000)],
	_86342::integer(0, 10),
	_86396::integer(0, 10),
	_86348::integer(0, 10),
	_86390::integer(0, 10),
	_86354::integer(0, 10),
	_86384::integer(0, 10),
	_86360::integer(0, 10),
	_86378::integer(0, 10),
	_86366::integer(0, 10),
	_86372::integer(0, 10).
By forcing earlier failure during the enumeration, the redundant constraint reduces the number of narrowing operations by over a factor of 3. This more than outweighs the additional cost of creating and checking that constraint.

One can take this even further by noting that `N` also equals the sum of `M*Mn`:
.pl
	magic_series3(N, Ks) :-
		length(Ks,N), Ks::integer(0,_),
		sum(Ks, S), {S==N},            % <== redundant constraint
		sumProd_(Ks, 0, M), {M==N},    % <== second redundant constraint
		magic_(Ks, 0, Ks).

	sumProd_([], _, 0).
	sumProd_([K|Ks], N, K*N+S):-  % summation of j*K(j), j=0,N-1
		N1 is N+1,
		sumProd_(Ks,N1,S).
leading to further factor of more than 2:
eg
	?- clpStatistics,magic_series3(10,M),findall(M,enumerate(M),Ms),clpStatistics(SS).
	﻿M = [_84802, _84808, _84814, _84820, _84826, _84832, _84838, _84844, _84850, _84856],
	Ms = [[6, 2, 1, 0, 0, 0, 1, 0, 0, 0]],
	SS = [userTime(0.0609150000000227), gcTime(0.002), globalStack(303072/1048544), trailStack(24872/526312), localStack(2432/118648), inferences(296604), narrowingOps(4965), narrowingFails(41), node_count(271), max_iterations(268/3000)],
	_84802::integer(0, 10),
	_84856::boolean,
	_84808::integer(0, 10),
	_84850::boolean,
	_84814::integer(0, 5),
	_84832::integer(0, 2),
	_84826::integer(0, 2),
	_84820::integer(0, 3),
	_84838::boolean,
	_84844::boolean.
Note that the solution space is considerably reduced (see domains of `M`) even before the enumeration step.

The lesson here is that even though redundant constraints may not be necessary to finding solutions, they can considerably speed up the enumeration process and may even make it unnecessary.

<#TableOfContents>

#### Bin Packing

A bin packing problem is one where one is given an assortment of objects of different types which are to be grouped into "bins" and where there are restrictions on the number and type of objects that can be placed in a bin. Such problems are often good abstract models for practical problems arising in the configuration of complex systems, and nicely illustrate the interaction of boolean and (usually) integer constraints.

The challenge of using constraints on such problems is often that of mapping a symbolic or informal description to numeric constraints. As an example consider packing items which may be glass, plastic, steel, wood, or copper into red, green, and blue bins subject to the following constraints:
.pl
	requires(wood, plastic).        % a wood item requires a plastic item in the same bin
	excludes(glass, copper).        % a copper item can't be in the same bin as a glass item
	excludes(copper, plastic).      %    or a plastic item

	capacity(red, 3).               % total number of items in each type of bin
	capacity(blue, 1).
	capacity(green, 4).

	capacity(red, wood, 1).         % at most 1 wood item in any red bin
	capacity(red, steel, 0).        % no steel items in a red bin
	capacity(red, plastic, 0).      % etc.
	capacity(green, wood, 2).
	capacity(green, glass, 0).
	capacity(green, steel, 0).
	capacity(blue, wood, 0).
	capacity(blue, plastic, 0).
	
If the red, green and blue bin types are mapped to integers 1 to 3, then booleans can be used "condition" the sum constraints. Further `requires`, `excludes` and the capacity constraints can be mapped to constraints on numeric values. The definition of a bin with `Type`, `Contents` (a list of integers, one for each type of commodity), and `Total`, the number of items in the bin, can be specified by:
.pl
	bin_types([red,green,blue]).

	commodities([glass, plastic, steel, wood, copper]).

	bin(Type, Contents, Total):-
		Type::integer(1,3), [Red,Green,Blue]::boolean,
		{Red == (Type==1), Green==(Type==2), Blue==(Type==3)},
		Contents=[Glass,Plastic,Steel,Wood,Copper], Contents::integer(0,_),
		{Binsize is Red*3 + Blue*1 + Green*4},            % "conditional" expression
		Binsize::integer(1,4),                            % redundant constraint: range of bin capacities
		{Total is Glass + Plastic + Steel + Wood + Copper},
		{Total>=1, Total =< Binsize },
		{(Wood >= 1) -> (Plastic >= 1)},                  % requires(wood,plastic).
		{Glass * Copper == 0},                            % excludes(glass,copper).
		{Copper * Plastic == 0},                          % excludes(copper,plastic)
		{Blue  -> (Wood + Plastic == 0)},                 % capacity(blue,wood,0) & capacity(blue,plastic,0).
		{Red   -> ((0==Steel + Plastic) and (Wood=<1))},  % capacity(red,wood,1) & capacity(red,steel,0) & capacity(red,plastic,0).
		{Green -> ((0==Glass + Steel) and (Wood=<2))}.    % capacity(green, wood,2) & capacity(green,glass,0) & capacity(green,steel,0).
Note that the redundant constraint `Binsize::integer(1,4)` specifying the range of bin sizes, has been added. Although perhaps obvious, it has a dramatic affect on performance as discussed below.

This can be checked most easily by enumerating the possibilities:
eg
	?- bin(T,C,A),enumerate([T,A|C]),writeln([T,A,C]),fail.
	[1,1,[0,0,0,0,1]]
	[1,1,[1,0,0,0,0]]
	[1,2,[0,0,0,0,2]]
	[1,2,[2,0,0,0,0]]
	[1,3,[0,0,0,0,3]]
	[1,3,[3,0,0,0,0]]
	[2,1,[0,0,0,0,1]]
	[2,1,[0,1,0,0,0]]
	[2,2,[0,0,0,0,2]]
	[2,2,[0,1,0,1,0]]
	[2,2,[0,2,0,0,0]]
	[2,3,[0,0,0,0,3]]
	[2,3,[0,1,0,2,0]]
	[2,3,[0,2,0,1,0]]
	[2,3,[0,3,0,0,0]]
	[2,4,[0,0,0,0,4]]
	[2,4,[0,2,0,2,0]]
	[2,4,[0,3,0,1,0]]
	[2,4,[0,4,0,0,0]]
	[3,1,[0,0,0,0,1]]
	[3,1,[0,0,1,0,0]]
	[3,1,[1,0,0,0,0]]
	false.

The program takes as input a list of items of the form `Count*Type` and produces a list of bins packed with those items. Clearly, many solutions are possible including one where each bin contains a single item, but we're just interested in the first solution.
.pl
	pack(Items, Bins) :-
		commodities(Names),                                 % process input,
		get_items_(Names,Items,Contents,0,Total),           % map to list of item counts and a total
		pack_(Total,Contents,BinsRaw),                      % setup constraints,
		enum_bins_(BinsRaw),                                % enumerate,
		!,                                                  % take first solution
		format_bins_(BinsRaw,Bins).                         % and format
	
	get_items_([],_,[],Total,Total).
	get_items_([N|Names],ItemsList,[C|Counts],Acc,Total):-
		(memberchk(C*N,ItemsList) -> NewAcc is Acc+C ; NewAcc=Acc),
		get_items_(Names,ItemsList,Counts,NewAcc,Total).
		
	pack_(0, [0,0,0,0,0], []).                              % no residual items
	pack_(Total, Amounts, [[Type,Contents,Size]|Bins]) :-
		bin(Type,Contents,Size),                            % constraints on one bin
		{T == Total - Size, T>=0},                          % reduce Total by number of items in the bin
		subtract_(Amounts, Contents, Residual),             % reduce each commodity count for items in the bin
		pack_(T, Residual, Bins).                           % and repeat until counts are 0

	subtract_([], [], []).
	subtract_([X|Xs], [Y|Ys], [Z|Zs]) :- {Z is X - Y, Z>=0},
		subtract_(Xs,Ys,Zs).

	enum_bins_([]).
	enum_bins_([[T,C,S]|Bs]):- 
		enumerate([T|C]),
		enum_bins_(Bs).

	format_bins_([],[]).
	format_bins_([[T,Contents,_]|BinsRaw],[Bin|Bins]):-
		bin_types(BTs), nth1(T,BTs,Type),
		commodities(Names), format_commodities_(Names,Contents,Comms),
		Bin =.. [Type|Comms], !,
		format_bins_(BinsRaw,Bins).

	format_commodities_([],[],[]).
	format_commodities_([N|Names],[Count|Contents],[Count*N|Comms]) :- Count>0, !,
		format_commodities_(Names,Contents,Comms).
	format_commodities_([N|Names],[_|Contents],Comms) :-
		format_commodities_(Names,Contents,Comms).

Note that `pack_/3` will keep adding bins until all constraints can be satisfied with residual amounts of `0`. Failure in enumeration forces this backtracking to occur. When a slotution is found it will be "optimal" in the sense that there are no solutions which use fewer bins. (There may be other solutions that use the same number of bins.) One reason the redundant constraint is effective (reduces the count of `narrowingOps` required by approximately a factor of 4), is that it quickly fails for a small number of bins:
eg
	?- clpStatistics,pack([3*glass,4*plastic,1*steel,4*wood,2*copper],Bins),clpStatistics(SS).
	﻿Bins = [red(2*copper), red(3*glass), green(2*plastic, 2*wood), green(2*plastic, 2*wood), blue(1*steel)],
	SS = [userTime(0.25506900000000066), gcTime(0.001), globalStack(519480/1048544), trailStack(166080/264168), localStack(1992/118648), inferences(1334656), narrowingOps(24803), narrowingFails(431), node_count(280), max_iterations(131/3000)].
Also note that only the bin type and contents need be enumerated since the total number of items in the bin is pre-determined by the contents.

There is a symmetry issue since the bins could have been listed in any order. To mitigate against this, the list of bins can be sorted, e.g., by number of items in the bin and type, before enumeration. Because enumeration begins at the low end, it is best to make this an ascending sort.
.pl
	ordpack(Items, Bins) :-
		commodities(Names),                                 % process input,
		get_items_(Names,Items,Contents,0,Total),           % map to list of item counts and a total
		pack_(Total,Contents,BinsRaw),                      % setup constraints,
		order(BinsRaw),                                     % order constraint
		enum_bins_(BinsRaw),                                % enumerate,
		!,                                                  % take first solution
		format_bins_(BinsRaw,Bins).                         % and format

	order([_]) :- !.   % finish with 1 bin
	order([X,Y|Xs]) :-
		order_(X,Y),   % order constraint on first two bins
		order([Y|Xs]). % order second and remaining bins

	order_([T1,_,S1], [T2,_,S2]) :- {(T1<T2) or ((T1==T2) and (S1=<S2))}.
This requires significantly less `narrowingOps` than the first version:
eg
	?- clpStatistics,ordpack([3*glass,4*plastic,1*steel,4*wood,2*copper],Bins),clpStatistics(SS).
	﻿Bins = [red(2*copper), red(3*glass), green(2*plastic, 2*wood), green(2*plastic, 2*wood), blue(1*steel)],
	SS = [userTime(0.165559), gcTime(0.0), globalStack(454112/1048544), trailStack(146328/264168), localStack(1992/118648), inferences(869599), narrowingOps(16453), narrowingFails(271), node_count(300), max_iterations(138/3000)].
These solutions are fairly effective for small numbers of bins, but do not scale well when the number of items is large and the bin sizes are small. A different approach uses `setof/3` to generate a sorted list (from smallest to largest capacity) of all the 22 possible bin configurations:
eg
	?- setof([S,T,C],(bin(T,C,S),enumerate([T|C])),Bins),length(Bins,BL).
	﻿Bins = [[1, 1, [0, 0, 0, 0, 1]], [1, 1, [1, 0, 0, 0, 0]], [1, 2, [0, 0, 0, 0, 1]], [1, 2, [0, 1, 0, 0, 0]], [1, 3, [0, 0, 0, 0, 1]], [1, 3, [0, 0, 1, 0, 0]], [1, 3, [1, 0, 0, 0, 0]], [2, 1, [0, 0, 0, 0, 2]], [2, 1, [2, 0, 0, 0, 0]], [2, 2, [0, 0, 0, 0, 2]], [2, 2, [0, 1, 0, 1, 0]], [2, 2, [0, 2, 0, 0, 0]], [3, 1, [0, 0, 0, 0, 3]], [3, 1, [3, 0, 0, 0, 0]], [3, 2, [0, 0, 0, 0, 3]], [3, 2, [0, 1, 0, 2, 0]], [3, 2, [0, 2, 0, 1, 0]], [3, 2, [0, 3, 0, 0, 0]], [4, 2, [0, 0, 0, 0, 4]], [4, 2, [0, 2, 0, 2, 0]], [4, 2, [0, 3, 0, 1, 0]], [4, 2, [0, 4, 0, 0, 0]]],
	BL = 22.
Backtracking is now confined to the enumeration step, which now is just concerned with how many of each bin definition is required to hold the total contents:
.pl
	fastpack(Items, Bins) :-
		commodities(Names),                                           % process input,
		get_items_(Names,Items,Contents,0,Total),                     % map to list of item counts and a total
		setof([S,T,C], (bin(T,C,S),enumerate([T|C])), BinDefs),       % set of bin definitions
		summation(BinDefs, Ns, NB, Total, [Glass,Plastic,Steel,Wood,Copper]),  % constraints with counts of each def.
		enumerate(Ns),       % enumerate the counts of each bin definition
		!,                   % first solution
		compress(NB,NBins).  % remove definitions with count=0

	summation([], [], [], 0, [0,0,0,0,0]).
	summation([[Sz,T,Cn]|Bs], [N|Ns], [(N*[T,Cn,Sz])|Xs], Tot, [G,P,S,W,C]) :-
		N::integer(0,_),
		Cn=[Glass,Plastic,Steel,Wood,Copper],
		{	T1 == Tot - N*Sz,
			G1 ==   G - N*Glass,
			P1 ==   P - N*Plastic,
			S1 ==   S - N*Steel,
			W1 ==   W - N*Wood,
			C1 ==   C - N*Copper
		},
		summation(Bs ,Ns ,Xs, T1, [G1,P1,S1,W1,C1]).

	compress([], []).
	compress([(0*_)|Xs], Ys):- !,  % discard unused definitions
		compress(Xs,Ys).
	compress([(N*[S,T,C])|Xs], [(N*Bin)|Ys]):-
		format_bins_([[T,C,S]],[Bin]),  % use existing bin formatting utility
		compress(Xs,Ys).
This version has excellent performance characteristics with minimal sensitivity to the number of bins required:
eg
	?- clpStatistics,fastpack([3*glass,4*plastic,1*steel,4*wood,2*copper],Bins),clpStatistics(SS).
	﻿Bins = [1*blue(1*steel), 1*green(2*copper), 1*red(3*glass), 2*green(2*plastic, 2*wood)],
	SS = [userTime(0.02092900000000064), gcTime(0.001), globalStack(440472/1048544), trailStack(156392/264168), localStack(1992/118648), inferences(95547), narrowingOps(1392), narrowingFails(0), node_count(211), max_iterations(422/3000)].

	?- clpStatistics,fastpack([32*glass,44*plastic,11*steel,44*wood,230*copper],Bins),clpStatistics(SS).
	﻿Bins = [11*blue(1*steel), 1*red(2*glass), 10*red(3*glass), 2*green(3*copper), 56*green(4*copper), 22*green(2*plastic, 2*wood)],
	SS = [userTime(0.029797999999999547), gcTime(0.002), globalStack(496512/524256), trailStack(186120/264168), localStack(1992/118648), inferences(134669), narrowingOps(2242), narrowingFails(0), node_count(211), max_iterations(832/3000)].

	?- clpStatistics,fastpack([132*glass,414*plastic,1001*steel,414*wood,230*copper],Bins),clpStatistics(SS).
	﻿Bins = [1001*blue(1*steel), 44*red(3*glass), 2*green(3*copper), 56*green(4*copper), 207*green(2*plastic, 2*wood)],
	SS = [userTime(0.05003599999999864), gcTime(0.004), globalStack(215240/524256), trailStack(42824/264168), localStack(1992/118648), inferences(221446), narrowingOps(4071), narrowingFails(0), node_count(211), max_iterations(2654/3000)].
Note that the number of `narrowingOps` has been drastically reduced (factor of 20) from the previous versions. However, it's not so clear in this case that the first solution found is one with the minimal number of bins. But if one considers the following:
1..
	- `enumerate/1` is a depth first enumeration of the list. The last item is fully "enumerated" before backtracking to the second last, etc.
	- the bin configurations are ordered (using `setof`) from smallest to largest.
Therefore, solutions are generated using the largest capacity bins first. Given the first solution, a better solution (fewer bins) can only occur if two or more bins in that solution can be combined into fewer bins, but that can only be done using at least one bin of a larger capacity. But all the solutions with larger capacity bins have already been enumerated (and failed) so there can't be a solution with fewer bins. (Again, there may be another solution with the same number of bins.)

This example is interesting for a number of reasons:
1..
	- It exploits a mixture of boolean and integer constraints (see `bin/3`).
	- It demonstrates again the value of redundant constraints (first two versions).
	- Different design approaches can have radically different performance chracteristics (no real surprise there), again use `clpStatistics`, in addition to Prolog's built-in tools (debuggers and profilers) for analysis.

<#TableOfContents>

#### Timing Analysis of Digital Circuits

Interface timing verification is the problem of verifying that the interfaces of various system components in a digital circuit can be connected and operate correctly. Each interface is given as a collection of events and constraints relating the occurrence times of these events. Verifying correct operation by exhaustive simulation is generally impractical, so some form of analytical approach is required. Even this simplistic description of the problem suggest that a CLP approach based on intervals (timing windows) might be advantageous.

A common example in the literature, e.g., [Solving linear, min and max constraint systems using CLP based on relational interval arithmetic], is the read cycle of a 8086 CPU in "Minimum" system configuration to a 2716 EPROM. From the [8086 Datasheet], the hardware configuration looks like:
[8086ReadCircuit]
Although this example is common in academic papers, the specifics of the circuit being analysed are unclear. So rather than rely on the intermediate timing graph from the papers, the approach described here goes back to the original information in the component datasheets:
*..
	* CPU : [8086 Datasheet]
	* EPROM : [2716 Datasheet]
	* Address Latch : [8282 Datasheet]
	* Bus Transceiver : [8286 Datasheet]
The timing characteristics of the devices typically are compose of a wave diagram and a table of timing values whose labels refer to the wave diagram. In this particular case the 8086 wave diagram for a read operation is:
[8086ReadWave]
The actual table of timing values is available in the 8086 datasheet and won't be reproduced here, but the relevant values will be used in the code below.

In addition, a common solution in the literature is to optimize the separation of event times which can be an NP-complete problem. However, the underlying objective is to prove that the timing specification is consistent and all timing requirements are met when components are inter-connected. So the solution described below directly tackles these objectives in a (hopefully) more transparent way while provided additional diagnostic information when these objectives aren't met.

For the purposes of timing analysis, capturing the circuit properties can be broken down into two steps. The first step is to identify the timing events and give them a label and a type. The type of event is necessary because any particular event may be dependent on more than one "parent" event. In some cases these are simple constraints which must be mutually satisified, but in others the event may depend on the earliest or latest parent. The names corresponding to these types are `lin` (mutual constraints), `min` (earliest parent), and `max` (latest parent).

The events themselves are things like clocks, address available on CPU address bus, etc. For the purposes of this example, some signals and associated events will be ignored, e.g., M/IO, Status controls, etc. The events are declared in a list containing elements of the form: `Type(Label)`:
.pl
	events([
		lin(c1),     % **Reference** falling edge of clock 1 (T1)
		lin(c2),     % falling edge of clock 2 (T2)
		lin(ce),     % falling edge of final clock in the cycle (T4)
		lin(a1),     % address available from CPU
		lin(a2),     % address unavailable from CPU
		lin(r1),     % read strobe (RD) falling edge
		lin(r2),     % read strobe (RD) rising edge
		lin(l1),     % address latch (ALE) rising edge
		lin(l2),     % address latch (ALE) falling edge
		max(mA1),    % address available to EPROM
		lin(r1),     % read strobe (RD) falling edge
		lin(r2),     % read strobe (RD) rising edge
		max(d1),     % data available from EPROM
		min(d2),     % data unavailable from EPROM
		lin(rD1),    % data available to CPU
		lin(rD2)     % data unavailable to CPU
	]).
The first event in the list is designated as the *reference* event and is assigned time value 0. All other event times are relative to the reference event time. 

For analysis purposes, a "net list" is created from the event list by adding an event time represented by an `integer` interval representing the earliest and latest time that the event can occur:
.pl
	% Create Net list from list of event names
	label_events([], []).
	label_events([Ev|Events], [ev(Type,Name,Time)|Net]) :-
		Ev =.. [Type,Name],
		Time::integer,
		label_events(Events, Net).
The second step in defining a network for analysis is to apply constraints to the event times based on the information in the component datasheets. There are actually two kinds of timing relationships. The first kind deals with causal relationships, e.g., "the address from the CPU becomes valid between 10 and 100 ns. after the leading edge of clock 1". (A value of 0 is assumed if no minimum time is given; an unspecified maximum can be represented by a variable.) These are "hard" constraints specified by the component manufacturer.

The second kind of constraint defines setup and hold times between components; these will be called "hazards" since violating these relationships indicate the circuit as defined fails to meet the timing requirements specified in the datasheets. These are design constraints used to validate correct circuit operation and, unlike hard constraints, cannot be used to used to narrow other event times.

A causal relationship (hard constraint) will be represented by a rule of the form `timing(Ev1,Ev2,[Min,Max])` which says event `Ev2` will follow parent event `Ev1` by a time interval between `Min` and `Max` ns. (or whatever common timing unit is implied). A hazard is represented by a rule of the form `hazard(Ev,SEv+ST,HEv-HT)` where `ST` is the setup time between `SEv` and `Ev` and `HT` is the hold time between `Ev` and `HEv`. 

The address latch is a simple component that requires both kinds of timing relationships:
.pl
	% 8282 Octal Latch
	timing(a1,mA1,[0,35]).    % TIVOV (Input to Output Delay)
	timing(l1,mA1,[0,55]).    % TIVOV (Input to Output Delay)
	
	hazard(l2,a1+0,a2-25).    % TIVSL/TSLIX (Input to Strobe Setup/Hold Time)
The `timing` rules capture the delay between the inputs `a1` (address start) and `l1` (latch start) and the output (`mA1`). The `hazard` rule indicates the setup/hold requirements for `l2` (latch end) on the address from the CPU.

To facilitate experimentation, two additional rules to define the clock characteristics and wait cycles are added. (For simplicity, the circuitry to correctly control the insertion of wait cycles has been assumed.) The complete set of timing information is below; the comments refer to timing labels in the datasheets:
.pl
	:- discontiguous timing/3,hazard/3.  % info for SWIP compiler
	% 8086 CPU
	clock_period(TCLCL,TCLCH,TCHCL) :-  % Clock Period, Clock Low Duration, Clock High Duration
		% Minimum values: 200 ns, 118 ns., 69 ns. from datasheet
		TCLCL=200, LL=118, HL=69,
		LH is TCLCL-HL, HH is TCLCL-LL,
		TCLCH=[LL,LH], TCHCL=[HL,HH].
	wait_cycles(0).     % number of wait cycles injected between T3 and T4
	
	timing(c1,c2,[T,T]) :-    % T2 clock begin
		clock_period(T,_,_).
	timing(c2,ce,[T,T]) :-    % Tend (T4) clock begin
		clock_period(C,_,_), wait_cycles(W), T is C*(2+W).
	timing(c1,a1,[10,110]).   % TCLAV (Address Valid Delay)
	timing(c1,l1,[0,80]).     % TCLLH (ALE Start)
	timing(l1,l2,[L,H]) :-    % TLHLL=TCLCH-20 (ALE End)
		clock_period(_,[LL,LH],_), L is LL-20, H is LH-20.
	timing(a1,l2,[L,H]) :-    % TAVAL=TCLCH-60
		clock_period(_,[LL,LH],_), L is LL-60, H is LH-60.
	timing(c1,l2,[L,H]) :-    % =TCLCH+TCHLL
		clock_period(_,[L,LH],_), H is LH+85.
	timing(c2,a2,[10,80]).    % TCLAX/TCLAZ (Address Hold Time, Address Float Delay)
	timing(c2,r1,[10,165]).   % TCLRL (Read Active Delay)
	timing(ce,r2,[10,150]).   % TCLRH (Read Inactive Delay)
	
	hazard(ce,rD1+30,rD2-10). % TDVCL/TCLDX (End Clock to Data Setup/Hold)
	
	% 8282 Octal Latch
	timing(a1,mA1,[0,35]).    % TIVOV (Input to Output Delay)
	timing(l1,mA1,[0,55]).    % TIVOV (Input to Output Delay)
	
	hazard(l2,a1+0,a2-25).    % TIVSL/TSLIX (Input to Strobe Setup/Hold Time)
	
	% 2716 EPROM
	timing(mA1,d1,[0,450]).   % tACC (Address to Output Delay)
	timing(r1,d1,[0,120]).    % tOE (Output Enable to Output Delay)
	timing(r2,d2,[0,100]).    % tOH/tOF (Output Disable to [Output Float])
	
	% 8286 Data Transceiver
	timing(d1,rD1,[0,35]).    % TIVOV (Input to Output Delay)
	timing(d2,rD2,[0,35]).    % TIVOV (Input to Output Delay)
Note that the `l2` event (end of address latch enable) is specified in the datasheet by three separate timing relationships relative to `l1`, `a1`, and `c1`. These will just be treated as three independant constraints (`l2` is event type `lin`) resulting in the narrowest possible event time for `l2` (see below).

This example just has two hazards to consider: address/control for loading the address latch and data available to the 8086 CPU.

aside> Aside: some published papers use the term "produced timing" for `timing/3` constraints and "required timing" for hazards. One might be tempted to also treat hazards as constraints. While this may be possible, it's difficult to diagnose problems since failure could be caused by either inconsistent `timing` defintions (rare) or unsatisfied requirements (the common case). Hence, the approach described here separates *definition* and *verification* into two distinct steps.

The process of timing analysis requires that the network be defined, which will include checking that the `timing` rules are consistent. The second step validates the circuit by checking the hazard conditions.

The pocess of defining the network involves creating the `Net` list and setting the reference event time to 0. The constraints implied by the `timing` rules are then applied. Any failure is an indication that the timing rules are inconsistent (an event time interval is empty) and a diagnostic message is output.
.pl
	define_network(Evs, Net) :-
		label_events(Evs, Net),  % from above
		Net = [ev(_,_,0)|_],     % designate the first event to be the reference event at T=0
		constrain_net_(Net,Net).
	
	% Apply timing constraints to each Net list entry
	% Note the use of memberchk/2 to access event information in Net list.
	constrain_net_([], _).
	constrain_net_([ev(Type,Ev,TEv)|Ds], Net) :-  % for each target event in the list
		% findall ancestors and keep time and edge delta
		findall(delta_(Par,Range),timing(Par,Ev,Range),TLs),
		% constrain based on event type, ouput dignostic on failure, but continue processing
		(constrain_type(TLs,Type,TEv,Net) -> 
			true
		; 
			writeln(timing_INCONSISTENCY(ev(Type,Ev),TLs))
		),
		constrain_net_(Ds,Net).
		
	constrain_type([],_,_,_) :- !.
	constrain_type([delta_(Par,[L,U])|TLs],lin,TEv,Net) :- !,  % lin event
		memberchk(ev(_,Par,ParTime),Net),
		Delta::integer(L,U),
		{TEv == ParTime + Delta},
		constrain_type(TLs,lin,TEv,Net).
	constrain_type(TLs,Type,TEv,Net) :-                         % max/min event
		min_max(TLs,Type,Ms,Net),
		{TEv == Ms}.
		
	% collect a min/max expression; must be at least one item
	min_max([delta_(Par,[L,U])],Type,ParTime+Delta,Net) :- !,
		memberchk(ev(_,Par,ParTime),Net),  % single argument
		Delta::integer(L,U).
	min_max([TL|TLs],Type,MinMax,Net) :-
		MinMax =.. [Type,Delta,More],
		min_max([TL],Type,Delta,Net),  % argument 1
		min_max(TLs,Type,More,Net).    % argument 2
Constraints on `lin` event times are simply `{TEv == ParTime+Delta}` where `Delta` is an interval defined by the value in `timing` rules. Constraints on `min/max` events are the `min/max` of all parents `Delta` constraints. (Since `min/max` is arity two, a nested chain of `min/max` expressions must be constructred if there are more than two parent events.)

To confirm that all the requirements are met, the `verify` step (Step 3) checks that no `hazard` violation has occurred. A violation is detected when the event time window intersects with interval time defined by the setup and hold specifications in the `hazard` defintion. For this example, a violation will be reported to the Prolog console.
.pl
	verify(Net) :-  % for each hazard, check setup and hold times
		forall(hazard(Ev,Setup,Hold), verify_(Net,Ev,Setup,Hold)).
	
	verify_(Net,Ev,SEv+ST,HEv-HT) :-
		memberchk(ev(_,Ev,ETime),Net),                  % reference event time
		memberchk(ev(_,SEv,STime),Net),                 % setup event time
		memberchk(ev(_,HEv,HTime),Net),                 % hold event time
		(\+({(ETime<STime+ST) or (ETime>HTime-HT)})     % assume integers so <,> are complete
		 ->	true    % no violation
		 ;	(print_interval(verify_FAIL(event(Ev,ETime),setup(SEv,STime+ST),hold(HEv,HTime-HT))),
			 nl
			)
		).
	verify_(Net,Ev,SEv+ST,HEv-HT) :-
		memberchk(ev(_,Ev,ETime),Net),                  % reference event time
		memberchk(ev(_,SEv,STime),Net),                 % setup event time
		memberchk(ev(_,HEv,HTime),Net),                 % hold event time
		(\+({(ETime<STime+ST) or (ETime>HTime-HT)}) ->  % assume integers so <,> are complete
			true    % no violation
		;
			print_interval(verify_FAIL(event(Ev,ETime),setup(SEv,STime+ST),hold(HEv,HTime-HT))), nl,
		).
Note that Prolog negation (`\+`) is used in checking the hazard timing, i.e., success means the event did not fall into either the setup or hold window. This ensures that other event times are not changed (narrowed). `print_interval` is used to output embedded intervals in as their domains.

Analysing the timing operation of a nominal circuit (5 Mhz. clock, 0 wait states):
eg
	?- events(Es),define_network(Es,Net),verify(Net),nl,﻿print_interval('Net'=Net),nl,nl.
	﻿verify_FAIL(event(ce,600),setup(rD1,integer(210,630)+30),hold(rD2,integer(610,885)-10))
	
	Net=[ev(lin,c1,0),ev(lin,c2,200),ev(lin,ce,600),ev(lin,a1,integer(47,110)),ev(lin,a2,integer(210,280)),ev(lin,r1,integer(210,365)),ev(lin,r2,integer(610,750)),ev(lin,l1,integer(7,80)),ev(lin,l2,integer(118,181)),ev(max,mA1,integer(47,145)),ev(lin,r1,integer(210,365)),ev(lin,r2,integer(610,750)),ev(max,d1,integer(210,595)),ev(min,d2,integer(610,850)),ev(lin,rD1,integer(210,630)),ev(lin,rD2,integer(610,885))]
	
	Es = [lin(c1), lin(c2), lin(ce), lin(a1), lin(a2), lin(r1), lin(r2), lin(l1), lin(l2), max(mA1), lin(r1), lin(r2), max(d1), min(d2), lin(rD1), lin(rD2)],
	Net = [ev(lin, c1, 0), ev(lin, c2, 200), ev(lin, ce, 600), ev(lin, a1, _5040), ev(lin, a2, _5054), ev(lin, r1, _5068), ev(lin, r2, _5082), ev(lin, l1, _5096), ev(lin, l2, _5110), ev(max, mA1, _5124), ev(lin, r1, _5138), ev(lin, r2, _5152), ev(max, d1, _5166), ev(min, d2, _5180), ev(lin, rD1, _5194), ev(lin, rD2, _5208)],
	_5040::integer(47, 110),
	_5110::integer(118, 181),
	_5096::integer(7, 80),
	_5124::integer(47, 145),
	_5166::integer(210, 595),
	_5194::integer(210, 630),
	_5068::integer(210, 365),
	_5054::integer(210, 280),
	_5082::integer(610, 750),
	_5180::integer(610, 850),
	_5208::integer(610, 885),
	_5138::integer(210, 365),
	_5152::integer(610, 750).
A timing violation has been detected; the data read from the EPROM via the transceiver arrives too late to meet the setup requirements of the CPU. If the source is modified to inject a wait state (`wait_cycles(1)`), the violation is avoided:
eg
	﻿?- events(Es),define_network(Es,Net),verify(Net),nl,print_interval('Net'=Net),nl,nl.
	
	Net=[ev(lin,c1,0),ev(lin,c2,200),ev(lin,ce,800),ev(lin,a1,integer(47,110)),ev(lin,a2,integer(210,280)),ev(lin,r1,integer(210,365)),ev(lin,r2,integer(810,950)),ev(lin,l1,integer(7,80)),ev(lin,l2,integer(118,181)),ev(max,mA1,integer(47,145)),ev(lin,r1,integer(210,365)),ev(lin,r2,integer(810,950)),ev(max,d1,integer(210,595)),ev(min,d2,integer(810,1050)),ev(lin,rD1,integer(210,630)),ev(lin,rD2,integer(810,1085))]
	
	Es = [lin(c1), lin(c2), lin(ce), lin(a1), lin(a2), lin(r1), lin(r2), lin(l1), lin(l2), max(mA1), lin(r1), lin(r2), max(d1), min(d2), lin(rD1), lin(rD2)],
	Net = [ev(lin, c1, 0), ev(lin, c2, 200), ev(lin, ce, 800), ev(lin, a1, _26858), ev(lin, a2, _26872), ev(lin, r1, _26886), ev(lin, r2, _26900), ev(lin, l1, _26914), ev(lin, l2, _26928), ev(max, mA1, _26942), ev(lin, r1, _26956), ev(lin, r2, _26970), ev(max, d1, _26984), ev(min, d2, _26998), ev(lin, rD1, _27012), ev(lin, rD2, _27026)],
	_26858::integer(47, 110),
	_26928::integer(118, 181),
	_26914::integer(7, 80),
	_26942::integer(47, 145),
	_26984::integer(210, 595),
	_27012::integer(210, 630),
	_26886::integer(210, 365),
	_26872::integer(210, 280),
	_26900::integer(810, 950),
	_26998::integer(810, 1050),
	_27026::integer(810, 1085),
	_26956::integer(210, 365),
	_26970::integer(810, 950).
Another alternative is to use 0 wait states but relax the clock frequency specified in `clock_period/3`, e.g., to 220 ns. :
eg
	﻿?- events(Es),define_network(Es,Net),verify(Net),nl,print_interval('Net'=Net),nl,nl.
	
	Net=[ev(lin,c1,0),ev(lin,c2,220),ev(lin,ce,660),ev(lin,a1,integer(27,110)),ev(lin,a2,integer(230,300)),ev(lin,r1,integer(230,385)),ev(lin,r2,integer(670,810)),ev(lin,l1,integer(0,80)),ev(lin,l2,integer(118,201)),ev(max,mA1,integer(27,145)),ev(lin,r1,integer(230,385)),ev(lin,r2,integer(670,810)),ev(max,d1,integer(230,595)),ev(min,d2,integer(670,910)),ev(lin,rD1,integer(230,630)),ev(lin,rD2,integer(670,945))]
	
	Es = [lin(c1), lin(c2), lin(ce), lin(a1), lin(a2), lin(r1), lin(r2), lin(l1), lin(l2), max(mA1), lin(r1), lin(r2), max(d1), min(d2), lin(rD1), lin(rD2)],
	Net = [ev(lin, c1, 0), ev(lin, c2, 220), ev(lin, ce, 660), ev(lin, a1, _26598), ev(lin, a2, _26612), ev(lin, r1, _26626), ev(lin, r2, _26640), ev(lin, l1, _26654), ev(lin, l2, _26668), ev(max, mA1, _26682), ev(lin, r1, _26696), ev(lin, r2, _26710), ev(max, d1, _26724), ev(min, d2, _26738), ev(lin, rD1, _26752), ev(lin, rD2, _26766)],
	_26598::integer(27, 110),
	_26668::integer(118, 201),
	_26654::integer(0, 80),
	_26682::integer(27, 145),
	_26724::integer(230, 595),
	_26752::integer(230, 630),
	_26626::integer(230, 385),
	_26612::integer(230, 300),
	_26640::integer(670, 810),
	_26738::integer(670, 910),
	_26766::integer(670, 945),
	_26696::integer(230, 385),
	_26710::integer(670, 810).
This example is a natural fit between the problem's event timing windows and the underlying CLP/intervals model, necessitating only 50 lines of code plus the circuit description. In reality, most of the work is mapping the timing information in the datasheets' waveform diagrams and timing tables. Execution times for analysing this admittedly simple circuit is a few milliseconds because of the simple nature of the constraints which can be easily narrowed by the fixed point iteration mechanism. Where large numbers of events are required, it's usually possible to de-construct the circuit into sub-circuits which can be analysed independently. To verify correct operation of the whole circuit, recombine the individual results with the same techniques used at the component level in this simple example.

This example can be easily adapted to other memory devices by replacing the EPROM timing information with the corresponding information of the devices, e.g., [2142-3] static RAM (capable of running at the designated CPU clock speed without wait states):
.pl
	% 2142-3 Static Ram
	timing(mA1,d1,[0,300]).   % tACC (Address to Output Delay)
	timing(r1,d1,[20,100]).   % tOE (Output Enable to Output Delay)
	timing(r2,d2,[0,80]).     % tOH/tOF (Output Disable to [Output Float])
Analysis of write cycles would require additional events since the data originates from the CPU and the relevant hazard relates to the data setup and hold times of the static RAM.

See also [Design and Implementation of a Static Timing Analyzer using CLP(BNR)] for a more in-depth treatment of this subject.

<#TableOfContents>

#### Solving Polynomial Equations in one Variable

Finding the roots of a univariate polynomial equation is the same as asking what is the value of `X` given the constraint `{P(X)==0}`. (Some of this section was covered earlier in "Constraints on Real Intervals".) A simple quadratic with two real roots (1 and 3):
eg
	?- X::real,{X**2-4*X+3 == 0}.
	﻿X::real(0.9999999999999998,3.0000000000000027).
The interval result must include both roots. To produce a single value, the problem must be further constrained:
eg
	?- X::real(2,_),{X**2-4*X+3 == 0}.
	﻿X:: 3.00000000000000... .
or to find both:
eg
	?- X::real,{X**2-4*X+3 == 0},({X=<2};{X>=2}).
	﻿X:: 1.000000000000000... ;
	X:: 3.00000000000000... .
Since we're looking for point solutions, `solve/1` is useful rather than guess at addtional constraints to isolate roots:
eg
	?- X::real,{X**2-4*X+3 ==0},solve(X).
	﻿X:: 1.000000000000000... ;
	X:: 3.00000000000000... .
	
Formulas exist for solving equations of low degree, so this may hardly seem to be worth the trouble, but this same technique works for pretty much any polynomial:
eg
	?- X::real,{17*X**256+35*X**17-99*X == 0},solve(X).
	﻿X:: 0.0... ;
	X:: 1.00502789289401... .

	?- X::real,{35*X**256-14*X**17+X == 0},solve(X).
	﻿X:: -0.847943660827315... ;
	X:: 0.0... ;
	X:: 0.847943660827315... ;
	X:: 0.995842494200498... .

A fourth degree polynomial equation with two real roots:
eg
	?- X::real,{X**4-4*X**3+4*X**2-4*X+3 == 0},solve(X).
	﻿X:: 1.000000... ;
	X:: 1.0000000... ;
	X:: 3.000000... ;
	X:: 3.000000... ;
	false.
What happened here? The two roots (`X=1` and `X=3`) produced four answers. `solve`works by splitting intervals into pieces and discarding those pieces which provably do not contain solutions. Pieces which *may* contain solutions are split again, and the process is repeated. However to keep execution and time space under control, there is a minimum size interval which can be split as determined by the Prolog environment flag `clpBNR_default_precision` (default value=6) which roughly approximates the number of digits of precision. More precise answers can only be achieved by the basic fixed point iteration driven by the interval primitive operations (as indicated by the first example), or by changing the default precision (Prolog flag `clpBNR_default_precision`). 
eg
	﻿?- set_prolog_flag(clpBNR_default_precision,7).
	true.
	
	?- X::real,{X**4-4*X**3+4*X**2-4*X+3 == 0},solve(X).
	﻿X:: 1.00000000... ;
	X:: 3.0000000... ;
	false.
The flip side says that the precision limit can cause the iteration to terminate without disproving that a solution exists, which is the case for the second and fourth solution.

The CLP(BNR) predicate `absolve` can be used to alleviate this problem (at a cost in performance). It "nibbles" away at the boundaries of an interval that fail to contain a solution and often removes these "false positives":
eg
	﻿?- X::real,{X**4-4*X**3+4*X**2-4*X+3 == 0},solve(X),absolve(X).
	﻿X:: 1.00000000... ;
	X:: 3.00000000... ;
	false.
But it is data dependant; sometimes both mechanisms togther are insufficent, as in this polynomial equation with roots at 0, 3, 4, and 5:
eg
	﻿?- set_prolog_flag(clpBNR_default_precision,7),X::real,{X**4-12*X**3+47*X**2-60*X==0},solve(X),absolve(X).
	X:: 0.0... ;
	X:: 3.00000000... ;
	X:: 4.0000000... ;
	X:: 4.00000000... ;
	X:: 5.00000000... ;
	false.

The underlying cause of the generation of false positives is the multiple occurence of `X` in the constraint set (the *dependency* problem discussed previously) which limits the narrowing possible via fixed point iteration. In addition, the shape of the polynomial in the vicinity of the X axis may mean a small `X` interval corresponds to a larger interval value of the polynomial, in fact, large enough to include 0. And to make matters worse, it can take a considerable amount of work to generate these false positives.

The ultimate "pathological" case is {`x^2-2x+1=0`}. This is one of a family of similar quadratics that just reach the X axis, i.e., the X axis is a tangent to the bottom of the parobala. `solve` (and `absolve`) can spend a lot of time in the vicinity of the solution (in this case, {`x=1`}) trying to disprove solutions in small intervals of `X`:
eg
	?- X::real,{X**2-2*X+1==0}.
	X:: 1.00... .

	?- X::real,{X**2-2*X+1==0},solve(X),absolve(X).
	﻿X:: 0.99999524... ;
	X:: 0.9999972322... ;
	X:: 0.999997699... ;
	X:: 0.999997991... ;
	X:: 0.999998105... ;
	X:: 0.999998413... ;
	X:: 0.9999985... ;
	X:: 0.9999986... ;
	X:: 0.9999986... ;
	X:: 0.9999987... ;
	X:: 0.9999987... ;
	X:: 0.9999988... ;
			.
			.
			.
	﻿X:: 0.9999994... ;
	X:: 0.9999995... ;
	X:: 0.999999... ;
	X:: 0.9999996... ;
	X:: 0.9999997... ;
	X:: 0.9999997... ;
	X:: 0.9999998... ;
	X:: 0.9999998... ;
	X:: 0.999999... ;
	X:: 1.000000... ;
	X:: 1.0000001... ;
	X:: 1.0000001... ;
	X:: 1.0000002... ;
	X:: 1.000000... ;
	X:: 1.0000003... ;
	X:: 1.0000004... ;
	X:: 1.0000004... ;
	X:: 1.0000005... ;
	X:: 1.0000005... ;
			.
			.
			.
	﻿X:: 1.0000010... ;
	X:: 1.0000011... ;
	X:: 1.0000011... ;
	X:: 1.0000012... ;
	X:: 1.0000012... ;
	X:: 1.0000013... ;
	X:: 1.0000014... ;
	X:: 1.0000014... ;
	X:: 1.0000015... ;
	X:: 1.000001807... ;
	X:: 1.000001921... ;
	X:: 1.00000365... ;
	false.
Just entering the equation without using `solve` (first query) quickly generates a low precision answer (~3-4 digits) before hitting the "work" limit (Prolog flag `clpBNR_iteration_limit`). But using `solve` to achieve greater precision takes an excessively long time while, even using `absolve`, generating many, many false positives. So, for this particular case, the general fixed point iteration and `solve` technique are not very effective.

Of course had the constraint been entered in a factored form (eliminating the dependancy issue),  it takes just a few operations to converge (no solve necessary):
eg
	?- {(X-1)**2==0}.
	X = 1.

By applying these general mechanisms, a satisfactory solution can usually be found but as is often the case in using constraints, performance can often be improved (space and/or time) by adding constraints ([Algorithmic Power from Declarative Use of Redundant Constraints]). The [Mean value theorem] states that if {`f`} is a continuous function on a closed interval and differentiable over the interval:
.am  f(x) = f(x_0) + f'(xi)(x-x_0) " where " (x_0<=xi<=x)or(x<=xi<=x_0)
Roots are values of {`x`} when {`f(x)=0`} (zero crossings). This equation yields three constraints: the Taylor expansion equation and the two ranges of {`xi`}. An algorithm to use these constraints involves selecting an advantageous value of {`x_0`} and using it to split the {`x`} interval. Applying the constraints, where {`f(x)=0`} will succeed if there is a zero crossing (a root) within either (or both) of the sub-intervals. Then the process can be recursively applied to the sub-intervals until an error threshold is reached (like `solve`).

Here's one implementation:
.pl
	findroots(P,X) :-
		partial_derivative(P,X,D),         % must be differentiable
		% create templates for original polynomial and derivative
		copy_term([P,X],Poly),
		copy_term([D,X],Derv),
		% apply constraint f(X)=0
		{P==0},
		% generate minimum width criteria from flag
		current_prolog_flag(clpBNR_default_precision,Pr),
		Eps is 10**(-(Pr+1)),
		% taylor expansion search
		taylor_(X,Poly,Derv,Eps).

	taylor_(X,Poly,Derv,Eps) :-
		% continue if interval X wider than error criteria
		range(X,[XL,XH]),                  % reached Eps limit?
		catch(XH-XL > Eps, _, true),  !,   % catch in case of overflow (limit not reached)
		% calculate simple midpoint value for X0
		X0 is XL/2+XH/2,
		apply_taylor_(X,X0,Poly,Derv),
		taylor_(X,Poly,Derv,Eps).          % rinse and repeat
	taylor_(X,Poly,Derv,Eps).              % based on Eps, X is too small to split
	
	apply_taylor_(X,X0,Poly,Derv) :-
		% symbolically apply function
		copy_term(Poly,[FX0,X0]),          %  f(X0) = FX0
		copy_term(Derv,[DXi,Xi]),          % f'(Xi) = DXi
		% Taylor expansion constraints
		{0 == FX0 + (X-pt(X0)) * DXi},
		({X =< Xi, Xi =< pt(X0)} ; {pt(X0) =< Xi, Xi =< X}).

The Taylor expansion constraints (end of the first clause of `apply_taylor_/4`) are almost copied directly from the original equation, and the termination condition (width of `X` less than `Eps`) is fairly obvious, but what are all those `copy_term`s about? The polynomial expression, `P` (as a function of `X`), is provided as an input argument but it has to be used multiple times, each with a different value of `X`. For example, the primary constraint must equate {`f(x)`} to 0, while each recursive call to `taylor_/4` evaluates {`f(x_0)`} with {`x_0`} calculated from the current bounds of `X`. Each such evaluation requires a new copy of `P` and argument `X`; hence the `copy_term`s.

aside>
	Aside: This could be also be done using extralogical features such as `assertz` or globals (implementation dependent), but using ISO standard `copy_term` is simpler (no retract/delete required) and achieves the desired effect.

The derivative expression (generated by `partial_derivative/2`) also requires multiple instantiations and must be copied.

A final point: note the use of `pt(X0)` rather than just `X0`. This bypasses the "fuzz" normally applied to floating point values in constraint definitions. In this case you just need a single value in the range of `X` so no fuzzing is preferable. (The implementation of `solve` also uses `pt(N)`.)

The question is what does this added complexity buy over the simple `solve` technique described earlier. Clearly there are additional (redundant) constraints which must be enforced, and since almost all of the computation is fixed point iteration, there is some cost to do do this. When the simple `solve` technique works and converges well, it is more efficient. But for the more difficult cases where convergance is poor and/or false positives are produced, `findroots` has a distinct advantage. Corresponding examples from above:
eg
	?- X::real,findroots(X**2-4*X+3,X).
	﻿X:: 1.000000000000000... ;
	X:: 3.00000000000000... .

	?- X::real,findroots(X**4-4*X**3+4*X**2-4*X+3,X).
	﻿X:: 1.0000000000000... ;
	X:: 3.00000000... ;
	false.

	?- X::real,findroots(17*X**256+35*X**17-99*X,X).
	﻿X:: 0.0... ;
	X:: 1.00502789289401... .

	?- X::real,findroots(35*X**256-14*X**17+X,X).
	﻿X:: -0.847943660827315... ;
	X:: 0.0... ;
	X:: 0.847943660827315... ;
	X:: 0.995842494200498... .

	?- X::real,findroots(X**4-12*X**3+47*X**2-60*X,X).
	﻿X:: 0.0... ;
	X:: 3.0000000000000... ;
	X:: 4.00000000... ;
	X:: 5.0000000000... ;
	false.

	?- X::real,findroots(X**2-2*X+1,X).
	﻿X:: 1.0000000... ;
	X:: 1.000000011177171... ;
	false.
Even `findroots` has difficulty eliminating values in the near vicinity of the precise solution although it does a much better job than `solve`.

Comparing `solve` and `findroots` in terms of narrowing operations performed (should be relatively implementation and machine independent):
.myw
	.tsv
		Polynomial						`solve`			`findroots`
		`X**2-4*X+3`					`    780`		`  1351`
		`X**4-4*X**3+4*X**2-4*X+3`		`  22085`		` 46953`
		`17*X**256+35*X**17-99*X`		`    228`		`   377`
		`35*X**256-14*X**17+X`			`   2349`		`  1848`
		`X**4-12*X**3+47*X**2-60*X`		` 350290`		`106406`
		`X**2-2*X+1`					`2089236`		` 36424`
	&	// local styles for table
		@css
			table.my_array { border-collapse:collapse; }
			table.my_array tr td { background:whitesmoke; padding: 4px 16px; }
			table.my_array tr:nth-child(1) td { background:lightgray; }

When `solve` works reasonably well (arguably the first four examples), `findroots` is no worse than a factor of two less efficient. However, `findroots` in worst case scenarios can considerably outperform `solve` (almost a factor of 200 in the "pathological" last example) and does a better job at eliminating false positives (and the time to calculate them) due to precision limits. (Multiple roots at one value are usually problematical; as an exercise try `X**4-X**3-3*X**2+5*X-2` which has a root at `X= -2` and a triple root at `X=1`.)

This section described two ways of finding the roots of any polynomial. The first just defines a constraint based using poloynomial and then searches for solutions over the variable range. It is very general but can be slow and produce many false positives depending on the nature of the polynomial in the vicinity of the "`x-axis`". The second method uses the fact that polynomials can be differentiated so that a Taylor expansion equivalent can be defined to accelerate the fixed point convergance. Note that these techniques also work for extended polynomials where the coefficients need not be integers and the powers are not limited to positive integers.
eg
	﻿?- X::real,findroots(X**4-X**3-3*X**5r2+5.1*X-2,X).
	﻿X:: 0.52022398295616... ;
	X:: 1.06412129... ;
	X:: 2.14165237228026... ;
	false.

<#TableOfContents>

#### Solving Linear Systems

In theory, systems of linear equations are just constraints which should be amenable to the standard CLP techniques already described. In the real world, just as in solving for the roots of polynomials, things are not quite so simple. Consider the following two examples of simple linear systems in two variables:
eg
	?- [X,Y]::real,{X+2*Y==1,X-Y==1}.
	X:: 1.000000000000000...,
	Y::real(-1.1102230246251565e-16,5.551115123125783e-17).

	?- [X,Y]::real,{X+Y==1,X-Y==1}.
	﻿X::real(-1.0e+16,1.0e+16),
	Y::real(-1.0e+16,1.0e+16).
In the first example, the fixed point iteration immediately converged to an approximate solution (`X=1, Y=0`), while in the second (almost identical) example, the fixed point iteration did not converge at all. To help explain this observation, here are the graphs of the two systems:
[FPconvergance]
The red and blue lines are the equations and the solid black lines represent fixed point iteration steps. In the case of no convergance, these steps form a square box (equations intersect at 90 degrees). This means that a fixed point is quickly reached but no narrowing occurs. In the convergent case, each iteration step results in a narrowing of the `X` or `Y` interval, and a solution is found quite quickly. So the first issue is that fixed point convergence is data dependent. (Note this isn't unique to linear systems but it's easy to demonstrate it with these simple examples.)

Fortunately `solve/1` can be used overcome this deficiency:
eg
	?- [X,Y]::real,{X+Y==1,X-Y==1},solve(X).
	﻿X:: 1.00000000...,
	Y::real(-8.257253769627937e-10,8.257253769627937e-10).
and, in this case, it works equally well if you solve for `X` or `Y` or both. However this may not always be the case. While the solution found should always be the same, the time taken to find it may vary widely (sometimes an order of magnitude or more). The following "real-world" examples were taken from [SYSTEMS OF LINEAR EQUATIONS AND MATRICES].

##### Manufacturing: Production Scheduling
"Ace Novelty wishes to produce three types of souvenirs: types A, B, and C. To manufacture a type-A souvenir requires 2 minutes on machine I, 1 minute on machine II, and 2 minutes on machine III. A type-B souvenir requires 1 minute on machine I, 3 minutes on machine II, and 1 minute on machine III. A type-C souvenir requires 1 minute on machine I and 2 minutes each on machines II and III. There are 3 hours available on machine I, 5 hours available on machine II, and 4 hours available on machine III for processing the order. How many souvenirs of each type should Ace Novelty make in order to use all of the available time?"

This can be simply modeled by:
.pl
	ace_produce([A,B,C],[MI,MII,MIII]) :-
		[A,B,C]::integer(0,_),   % numbers of each type to produce
		{
		   MI == 2*A + B + C,    % jobs for machine I
		  MII == A + 3*B + 2*C,  % jobs for machine II
		 MIII == 2*A + B + 2*C   % jobs for machine III
		}.
Fixed point iteration alone is insufficient but `solve` quickly finds the solution:
eg
	?- ace_produce([A,B,C],[180,300,240]).
	A::integer(0,90),
	B::integer(0,100),
	C::integer(0,120).

	?- ace_produce([A,B,C],[180,300,240]),solve(A).
	A = 36,
	B = 48,
	C = 60 ;
	false.
So Ace Novelty should make 36 type-A souvenirs, 48 type-B souvenirs, and 60 type-C souvenirs.

##### Capital Expenditure Planning
The management of Hartman Rent-A-Car has allocated $1.5 million to buy a fleet of new automobiles consisting of compact, intermediate-size, and full-size cars. Compacts cost $12,000 each, intermediate-size cars cost $18,000 each, and full-size cars cost $24,000 each. If Hartman purchases twice as many compacts as intermediate-size cars and the total number of cars to be purchased is 100, determine how many cars of each type will be purchased. (Assume that the entire budget will be used.)
eg
	﻿?- [C,I,F]::integer(0,_),{C+I+F==100,12000*C+18000*I+24000*F==1500000,C==2*I}.
	﻿C::integer(0,100),
	I::integer(0,50),
	F::integer(0,62).

	﻿?- [C,I,F]::integer(0,_),{C+I+F==100,12000*C+18000*I+24000*F==1500000,C==2*I},solve(C).
	C = 60,
	I = 30,
	F = 10 ;
	false.
As above, the dependancy issue requires the use of `solve/1` to generate a solution.  

##### Simple D.C. Circuit Analysis
This example has been included because [Towards Practical Interval Constraint Solving in Logic Programming] documents it as a linear system problem that is not handled well (at all?) by the CLP techniques described so far. Consider the D.C. circuit:
[SimpleCircuit]
The problem as stated is to solve for the currents flowing through the resistors assuming {`V=10`} volts and {`R_i = i Omega "for" i=1,2,...,9`}. The arrows indicate the direction of positive current flow. For the most part they are somewhat arbitrary; a negative value just means the flow is in the opposite direction. Applying Kirchhoff's laws yields the following equations:
.am
	    I_s-I_1-I_2-I_8 = 0,                                 I_1 = 10,
	       -I_s+I_1+I_7 = 0,                   2I_2-3I_3-8I_8 = 0,
	           I_2+I_3-I_5 = 0,                3I_3+5I_5-9I_9 = 0,
	 -I_3-I_4+I_8-I_9 = 0,             -4I_4+6I_6+9I_9 = 0,
	           I_4+I_6-I_7 = 0,  -I_1+4I_4+7I_7+8I_8 = 0,
	           I_5-I_6+I_9 = 0
As the paper states, there are 11 equations in 10 unknowns, but it's not obvious where the redundancy occurs. (Further it is suggested that initial ranges of `[-100,100]` be used for all intervals, although except for one case, this is not really necessary.)

However consider {`I_s`}. From the equations, it is not obvious that it cannot be negative. But that implies that poitive current can flow from the negative to the positive terminals of the voltage supply. Furthermore any search, e.g., using `solve/1`, will spend considerable effort attempting to find solutions where this can occur. (The paper cited indicates either no answer was achieved within 24 hours on a 2 MIP CPU or no narrowing occurs even using `solve`.) But adding the simple constraint {`Is>=0`} generates the answer withing a second or two (machine dependent). Here's the CLP(BNR) program:
.pl
	simpleCircuit(Vs) :-
		simpleCircuitDef(Vs,EQs),  % EQs is list of constraint equations in Vs
		Vs::real,                  % decalre so intervals are finite
		{EQs}.                     % activate constraints

	simpleCircuitDef([Is,I1,I2,I3,I4,I5,I6,I7,I8,I9],
		[
		 Is-I1-I2-I8 == 0,                 I1 == 10,
		   -Is+I1+I7 == 0,     2*I2-3*I3-8*I8 == 0,
			I2+I3-I5 == 0,     3*I3+5*I5-9*I9 == 0,
		-I3-I4+I8-I9 == 0,    -4*I4+6*I6+9*I9 == 0,
			I4+I6-I7 == 0, -I1+4*I4+7*I7+8*I8 == 0,
			I5-I6+I9 == 0
		]).
Again, no narrowing occurs in the initial fixed point iteration, but it's not obvious which variables to use with `solve`. As `solve` takes a list of variables (breadth first across all intervals named) there's no reason not to use the entire set of interval values:
eg
	﻿?- Vs=[Is,I1,I2,I3,I4,I5,I6,I7,I8,I9],simpleCircuit(Vs).
	﻿Vs = [Is, 10, I2, I3, I4, I5, I6, I7, I8|...],
	I1 = 10,
	Is::real(-9.99999999999999e+15, 1.0e+16),
	I8::real(-6.25e+15, 6.25e+15),
	I2::real(-1.0e+16, 1.0e+16),
	I5::real(-1.0e+16, 1.0e+16),
	I9::real(-8.888888888888889e+15, 8.888888888888889e+15),
	I3::real(-1.0e+16, 1.0e+16),
	I4::real(-1.0e+16, 1.0e+16),
	I6::real(-1.0e+16, 1.0e+16),
	I7::real(-1.0e+16, 9.99999999999999e+15).
	
	?- Vs=[Is,I1,I2,I3,I4,I5,I6,I7,I8,I9],simpleCircuit(Vs),solve(Vs).
	﻿Vs = [Is, 10, I2, I3, I4, I5, I6, I7, I8|...],
	I1 = 10,
	Is:: 10.8282986...,
	I8:: 0.2592087...,
	I2:: 0.5690898...,
	I5:: 0.2572598...,
	I9:: 0.0389787...,
	I3:: -0.3118300...,
	I4:: 0.5320600...,
	I6:: 0.2962385...,
	I7:: 0.8282986... ;
	false.
Using `solve` finds the solution in a few seconds, but there is an arguably missing constraint. We are probably not interested in the case where `Is` is negative, i.e., current flows from the prositive terminal of the voltage source to the negative. Adding this contraint reduces the execution time by over a factor of three:
eg
	﻿?- Vs=[Is,I1,I2,I3,I4,I5,I6,I7,I8,I9],simpleCircuit(Vs),time(solve(Vs)).
	﻿% 14,300,256 inferences, 3.461 CPU in 3.477 seconds (100% CPU, 4131337 Lips)
	Vs = [Is, 10, I2, I3, I4, I5, I6, I7, I8|...],
	I1 = 10,
	Is:: 10.8282986...,
	I8:: 0.2592087...,
	I2:: 0.5690898...,
	I5:: 0.2572598...,
	I9:: 0.0389787...,
	I3:: -0.3118300...,
	I4:: 0.5320600...,
	I6:: 0.2962385...,
	I7:: 0.8282986... .
	
	?- Vs=[Is,I1,I2,I3,I4,I5,I6,I7,I8,I9],simpleCircuit(Vs),{Is>=0},time(solve(Vs)).
	﻿% 3,750,130 inferences, 1.011 CPU in 1.016 seconds (100% CPU, 3707978 Lips)
	Vs = [Is, 10, I2, I3, I4, I5, I6, I7, I8|...],
	I1 = 10,
	Is:: 10.828298...,
	I8:: 0.2592087...,
	I2:: 0.569089...,
	I5:: 0.257259...,
	I9:: 0.0389787...,
	I3:: -0.3118300...,
	I4:: 0.532060...,
	I6:: 0.296238...,
	I7:: 0.828298... .
This example demonstrates the importance of ensuring that all known constraints are explicit to minimize execution time, even if some of the constraints seem redundant. In this case `solve` can spend  much wasted time searching for soluitons for negative `Is`.

##### Under and Overdetermined Systems
The examples above are nicely behaved but what happens if the system of equations is under or overdetermined? In case of the latter, incompatible additional constraints should result in failure:
eg
	?- [X,Y]::real,{X+2*Y==4,X-2*Y==0,4*X+3*Y==12}.
	false.
In the under-determined case, little or no narrowing occurs regardless of how hard `solve` tries to find a solution:
eg
	?- [X,Y,Z,W]::real,{X+2*Y-3*Z+W== -2, 3*X-Y-2*Z-4*W==1, 2*X+3*Y-5*Z+W== -3}, solve([X,Y,Z,W]).
	﻿X::real(-1.0e+16, 1.0e+16),
	W::real(-1.0e+16, 1.0e+16),
	Y::real(-1.0e+16, 1.0e+16),
	Z::real(-1.0e+16, 1.0e+16).
In general, `solve` can take a long time to determine that little or no narrowing is possible if the set of linear equations is underdetermined.

##### Using Gaussian Elimination
As the examples above show, the general technique using `solve` (when necessary) with `{}` constraints can be used to solve systems of linear equations. Futhermore, there's nothing in this approach that is specific to linear systems. Non-linear equations and even inequalties can be specified without changing the basic strategy. That's not to say that this approach is the most efficient. When it is known that the problem is a linear system, standard numerical methods, e.g., Gaussian elimination, can be used with interval constraints to find solutions much faster than using `solve` (which, after all, is a pretty blunt instrument). Furthermore, these techniques can be used without undue concerns for accuracy; the mathematical correctness of the underlying arithmetic and the use of intervals contain any rounding errors in the answers.

To demonstrate this, assume that there is a predicate `rewrite_linear(EQs,LinearEQs,OtherEQs)` which takes a list of constraints, `EQs` and separates them into two lists: `LinearEQs` which contain the linear equations (standard defintion) and `OtherEQs`. Furthermore conventional Gaussian elimination has been used to rewrite the original linear equations into an upper triangular form. (See the Appendix 1 for the souce of `rewrite_linear/3` which is too large to include here.) A few examples:
eg
	?- rewrite_linear([2*X+4*Y+6*Z==22,3*X+8*Y+5*Z==27,-X+Y+2*Z==2],Cs,Os).
	Cs = [X+2*Y+3*Z==11,Y-2*Z== -3,Z==2].
	Os = [].
	
	?- rewrite_linear([3*X-2*Y+8*Z==9,-2*X+2*Y+Z==3,X*Y==W,X+2*Y-3*Z==8],Cs,Os).
	﻿Cs = [X-2r3*Y+8r3*Z==3, Y+19r2*Z==27r2, Z==1],
	Os = [X*Y==W].
The generated lists `Cs` and `Os` can then be used as input to `{}` to add them to the constraint network. In many cases this is sufficient to generate the solution:
eg
	﻿?- rewrite_linear([2*X+4*Y+6*Z==22,3*X+8*Y+5*Z==27,-X+Y+2*Z==2],Cs,Os),{Cs,Os}.
	X = 3,
	Y = 1,
	Z = 2,
	Cs = [3+2*1+3*2==11, 1-2*2== -3, 2==2],
	Os = [].

	﻿?- rewrite_linear([3*X-2*Y+8*Z==9,-2*X+2*Y+Z==3,X*Y==W,X+2*Y-3*Z==8],Cs,Os),{Cs,Os}.
	X = 3,
	Y = 4,
	Z = 1,
	W = 12,
	Cs = [3-2r3*4+8r3*1==3, 4+19r2*1==27r2, 1==1],
	Os = [3*4==12].
For a complete set of linear equations (at least `N` equations in `N` unknowns, the constraint system does little more than back substitution and evaluation to produce the solution.

The simple, slow convergant case from above:
eg
	?- rewrite_linear([X+Y==1,X-Y==1],Cs,Os).
	Cs = [X+Y==1,Y==0],
	Os = [].

	﻿?- rewrite_linear([X+Y==1,X-Y==1],Cs,_),{Cs}.
	X = 1,
	Y = 0,
	Cs = [1+0==1, 0==0].

Note that, in all these examples, the solutions are exact because floating point values introduced in splitting intervals (with `solve`) have been avoided.

Finally here is the "problematic" simple circuit example which now generates the solution in a few milliseconds without the use of `solve`:
eg
	﻿?- rewrite_linear([Is-I1-I2-I8 == 0,   I1 == 10,
		   -Is+I1+I7 == 0,     2*I2-3*I3-8*I8 == 0,
		    I2+I3-I5 == 0,     3*I3+5*I5-9*I9 == 0,
		-I3-I4+I8-I9 == 0,    -4*I4+6*I6+9*I9 == 0,
		    I4+I6-I7 == 0, -I1+4*I4+7*I7+8*I8 == 0,
		    I5-I6+I9 == 0],Cs,_).
	Cs = [Is-I1-I2-I8==0, I1==10, I2+I8-I7==0, I8-1r5*I7+3r10*I3==0, I7+13r8*I3-5r4*I5==0, I3+ ... * ... - 3*I9==0, ... - ... - ... * ... == 0, ... + ... == 0, ... == ...|...].
	
	?- rewrite_linear([Is-I1-I2-I8 == 0,   I1 == 10,
		   -Is+I1+I7 == 0,     2*I2-3*I3-8*I8 == 0,
		    I2+I3-I5 == 0,     3*I3+5*I5-9*I9 == 0,
		-I3-I4+I8-I9 == 0,    -4*I4+6*I6+9*I9 == 0,
		    I4+I6-I7 == 0, -I1+4*I4+7*I7+8*I8 == 0,
		    I5-I6+I9 == 0],Cs,_),{Cs}.
	Is = 55560r5131,
	I1 = 10,
	I2 = 2920r5131,
	I8 = 190r733,
	I7 = 4250r5131,
	I3 = -1600r5131,
	I5 = 1320r5131,
	I9 = 200r5131,
	I4 = 390r733,
	I6 = 1520r5131,
	Cs = [55560r5131-10-2920r5131-190r733==0, 10==10, 2920r5131+190r733-4250r5131==0, 190r733-1r5*4250r5131+3r10* -1600r5131==0, 4250r5131+13r8* -1600r5131-5r4*1320r5131==0, -1600r5131+ ... * ... - 3*200r5131==0, ... - ... - ... * ... == 0, ... + ... == 0, ... == ...|...].
Again, the solution values are precise since the input values (voltage source and resistance values) are precise and the gaussian pivoting produces rational coefficients. Rationals are a bit hard to comprehend as output, but they can be converted to (approximate) floating point values using `FIx is float(Ix)`. Or force the use of floats by setting `I1` to a floating point value:
eg

	﻿?- rewrite_linear([Is-I1-I2-I8 == 0,   I1 == 10.0,
		   -Is+I1+I7 == 0,     2*I2-3*I3-8*I8 == 0,
		    I2+I3-I5 == 0,     3*I3+5*I5-9*I9 == 0,
		-I3-I4+I8-I9 == 0,    -4*I4+6*I6+9*I9 == 0,
		    I4+I6-I7 == 0, -I1+4*I4+7*I7+8*I8 == 0,
		    I5-I6+I9 == 0],Cs,_),{Cs}.
	﻿Cs = [Is-I1-I2-I8==0, I1==10.0, I2+I8-I7== -0.0, I8-1r5*I7+3r10*I3== -0.0, I7+13r8*I3-5r4*I5==0.0, I3+ ... * ... - 3*I9==0.0, ... - ... - ... * ... == 0.0, ... + ... == 0.0, ... == ...|...],
	Is:: 10.8282985772753...,
	I8:: 0.25920873124147...,
	I7:: 0.8282985772753...,
	I3:: -0.31183005262132...,
	I5:: 0.25725979341259...,
	I9:: 0.03897875657766...,
	I4:: 0.5320600272851...,
	I6:: 0.296238549990255...,
	I2:: 0.5690898460339...,
	I1:: 10.00000000000000... .

In a real world example, precise input values for resistors and voltage are not practical, but imprecise values can be accomodated with intervals.

The use of Gaussian elimination has a dramatic effect on performance; in the case of the circuit example, over two orders of magnitude. In particular, time is not wasted trying to find solutions to underdetermined sets of equations by splitting intervals. As was the case for finding the roots of polynomials using Taylor expansion, this demonstrates how the use of standard numerical methods, taking advantage of problem specific properties, can be used with interval constraints to improve on the general search techniques provided. Furthermore, the combination is mathematically sound, unlike using these same tecniques with floating point values. The use of `rewrite_linear/3` (or similar) can also be used with mixed linear and non-linear constraints. The only restriction is the full set of linear constraints must be provided in the input list, i.e., they can't be incrementally added. By pushing this responsibilty up to the application level, the underlying basic constraint handling is greatly simplified. In general, numerical methods that depend on more than one constraint (equation) must be implemented outside the constraint system.

<#TableOfContents>

&
	[transitive closure] <- link https://en.wikipedia.org/wiki/Transitive_closure
	\\ [Boolean satisfiability problem] <- link https://en.wikipedia.org/wiki/Boolean_satisfiability_problem
	\\ [NP-complete]        <- link https://en.wikipedia.org/wiki/NP-completeness
	[`cardinality`]      <- link #cardinality
	[Petri nets]         <- link https://en.wikipedia.org/wiki/Petri_net
	[Siphons and Traps Structural Analysis Techniques Behaviour of a Petri Nets] <- link https://www.academia.edu/19468769/Siphons_and_Traps_Structural_Analysis_Techniques_Behaviour_of_a_Petri_Nets
	[Solving linear, min and max constraint systems using CLP based on relational interval arithmetic] <- link https://www.sciencedirect.com/science/article/pii/S0304397596001983
	// [timing_graph] <- image images/timing80286_2716.png width=50% height=50% style="margin-left:200px"
	// [Min-Max_Inequalities] <- linkhttps://www.researchgate.net/publication/220652141_Min-Max_Inequalities_and_the_Timing_Verification_Problem_with_Max_and_Linear_Constraints
	[8086 Datasheet] <- link supplemental/8086-datasheet.pdf target="_blank"
	[2716 Datasheet] <- link supplemental/2716-datasheet.pdf target="_blank"
	[8282 Datasheet] <- link supplemental/8282-datasheet.pdf target="_blank"
	[8286 Datasheet] <- link supplemental/8286-datasheet.pdf target="_blank"
	[2142-3] <- link supplemental/2142-datasheet.pdf target="_blank"
	[8086ReadCircuit] <- image images/EgMinConfig.png width=50% height=50% style="margin-left:200px"
	[8086ReadWave] <- image images/EgWaveforms.png width=50% height=50% style="margin-left:200px"
	[Design and Implementation of a Static Timing Analyzer using CLP(BNR)] <- link https://ridgeworks.github.io/BNRProlog-Papers/docs/StaticTimingAnalyzer/StaticTimingAnalyzer.html
	[Algorithmic Power from Declarative Use of Redundant Constraints] <- link http://webhome.cs.uvic.ca/~vanemden/Publications/redund.pdf
	[Mean value theorem] <- link https://en.wikipedia.org/wiki/Mean_value_theorem
	[FPconvergance]  <- image images/linearConvergance.png width=50% height=50% style="margin-left:200px"
	[SYSTEMS OF LINEAR EQUATIONS AND MATRICES] <- link https://docplayer.net/21711942-The-linear-equations-in-two-variables-studied-in.html
	[SimpleCircuit] <- image images/SimpleCircuit.png  width=50% height=50% style="margin-left:200px"
	[Towards Practical Interval Constraint Solving in Logic Programming] <- link https://pdfs.semanticscholar.org/1dca/e2a910184c4b2f9d770f054168150c6d0bde.pdf